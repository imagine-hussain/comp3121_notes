
\section{Introduction - Revision}

\subsection{Rates of Growth}

\paragraph{The Problem}

To analyse algorithms, we need a way to compare two functions representing
the runtime of each algorithm. However, comparing values directly presents a few issues.
\begin{itemize}
    \item Outliers may affect the results.
    \item One algorithm may be slower for a set period of time, but catch
    up after a while.
    \item The runtime of an algorithm may vary depending on the
    implementation or the architecture of the machine where, some
    instructions are faster than others.
\end{itemize}

\paragraph{Asymptotic Growth}
For algorithms, we often prefer to refer to them in terms of their
asymptotic growth in runtime, with relation to the input size.

A function that quadruples with every extra input will always have a
greater runtime than one that increases linearly with each new input, for
some large enough input size.

\paragraph{Big \(O\) Notation}
We say \(f(n) = O(g(n))\) if there exists a positive constants \(C, N\)
such that
\[
    0 \leq f(n) \leq C g(n) \quad \forall n \geq N.
\]

We may refer to \(g(n)\) to be the asymptotic upper bound for \(f(n)\).

\paragraph{Big Omega Notation}
We say \(f(n) = \Omega(g(n))\) if there exists positive constants \(c, N\) such that 
\[0 \leq c g(n) \leq f(n) \quad \forall n \geq N.\]

Then, \(g(n)\) is said to be an asymptotic lower bound for \(f(n)\).
It is useful to say that a problem is at least \(\Omega(g(n))\).

\paragraph{Landau Notation}
\(f(n) = \Omega (g(n))\) if and only if \(g(n) = O(f(n))\).

There are strict version of Big \(O\) and Big \(Omega\) notations; these are little \(o\) and little \(\omega\) respectively.

We say \(f(n) = \Theta(g(n))\) if
\[
    f(n) = O(g(n)) \text{ and } f(n) = \Omega(g(n)).
\]
That is, both \(f\) and \(g\) have the same asymptotic growth.


\paragraph{Logarithms}
Logarithms are defined so that for \(a, b > 0\) where \(n \neq 1\),
let \[ n = \log_a b \Leftrightarrow a^n = b.\]

They have the following properties:
\begin{itemize}
    \item \(a^{\log_a n} = n\)
    \item \(\log_a (mn) = \log_a (m) + \log_a(n)\)
    \item \(\log_a (n^k) = k\log_a (n)\)
\end{itemize}

By the change of base,
\[
    \log_a(x) = \frac{\log_b (x)}{\log_b (a)}.
\]
As such, the denominator is constant in terms of \(x\) and so all log bases are equivalent under Big Theta notation.

\subsection{Assumed Data Structures}

\paragraph{Arrays}
We assume static arrays (though, it is possible to extend to dynamic arrays).

\begin{itemize}
    \item We assume random-access in \(O(1)\)
    \item Insert / delete \(O(n)\)
    \item Search: \(O(n)\) - \(\log n\) if sorted
\end{itemize}

\paragraph{Linked Lists}
We assume the linked lists are doubly-linked since the \()2\times \)  overhead is negligible.

\begin{itemize}
    \item Accessing next / previous: \(O(1)\)
    \item Insert / delete to head or tail: \(O(1)\)
    \item Search: \(O(1)\).
\end{itemize}

\paragraph{Stacks}
Last in, first out.
\begin{itemize}
    \item Accessing top: \(O(1)\)
    \item Insert / delete from top: \(O(1)\)
\end{itemize}

\paragraph{Queue}
First in, first out.
\begin{itemize}
    \item Access front: \(O(1)\)
    \item Insert front: \(O(1)\)
    \item Delete front: \(O(1)\)
\end{itemize}

\paragraph{Hash Tables}
Store values by their hashed keys.
Ideally, no two keys will hash to the same value, however this may not be guaranteed.

\begin{itemize}
    \item Search is expected to be \(O(1)\) however, in the worst case, we expect to have to search through all the values in \(O(n)\).
    \item Insertion is expected to be \(O(1)\) however, in the worst case, we expect to have to search through all the values in \(O(n)\).
    \item Deletion follows the same pattern of \(O(1)\) expectation and \(O(n)\) worst case.
\end{itemize}

